Entering work. Welcome to hashing it out, a podcast where we talked to the tech innovators behind blocked in infrastructure and decentralized networks. We dive into the weeds to get at why and how people build this technology and the problems they face along the way. Come listen and learn from the best in the business so you can join their ranks. Welcome back, everybody, to hashing it out. As always, your host, Dr Corey Patty, and with me is my trust to go host, Calin Crouche. Say what's up, everybody calling? WHAT'S UP, everybody? Colin stays. Hosts. We have a repeat guest, Dr Emmond Goon Serier, coming back, coming back to talk us, talk to us about avalanche and Ava and the progress has recently been made within that space. Dr Sur want you first give us a quickid introduction, or reintroduction for that matter, as to who you are and how you join the space and what maybe I like a like the Tin Word Synopsis Of what AVA is. Sure the first of all, thank you very much for having me. My background is quite straightforward. I'm an academic. I am a professor at Cornell. I've been here for quite a while now. I started out working on peer to peer currencies back in two thousand and two I built something called Karma with proof of work mining in it, and for that was back then. It was meant to be a virtual currency for file sharing and file transfers. Later on I discovered one of the biggest flaws in Bitcoin, known as selfish mining, and got a lot of flak for it, but it is what it is and people came to accept that. It's now the the third most cited paper in the cryptocurrency space, after Bitcoin, etherorium. It's the next one is selfish mining. And let's see. Then I did things like Bitcoin M G, which is used in a bunch of the currencies like turnity and waves and a bunch of others, you know, with a some total valuation around to be or so, and what else is going on? So I did vaults, I worked on how to make coins more secure and most recently I branched out and I'm doing a company, and I'm only from Cornell and I'm doing a company called Alva laps and it's building on this cool new consensus for the call called Avalanche, which happens to be, I think, one of the biggest things that happened in distributed systems ever, one of the three biggest things that ever happened, I think, and so I can put that in context if the viewers would like to hear a little bit about the context in which, you know, all this research is being done. So, for one, I'd say, if you're unfamiliar with this, we've we've interviewed doctor's career before on the show. We went kind of in depth right when the white paper from team rocket was was was released, to go into death on the white paper and how things work. We're going to do something similar here, but then maybe a little more updated, because I'm a I'm assuming things have changed. It changed, you've updated the white paper, you have stress tests, there's an actual network and decisions have been made since, since back then on, I guess, how the architecture of how it all works together. So why don't we get start by just saying, like the consensus protocol, which is what you're referring to as avalanche, is one of the, you know, three most, I guess you've said, important things and distributed systems and ever, why is that and what is it? Okay, so if we roll the clock back to let's say, let's roll it back to the emergence of distribuling systems asn't as an area. So back to the S. what's happened there is people first started worrying about the two generals problem. And you know how two people can agree to conduct something online and know that the other party will go and go ahead and do it. That was happening and what I would call the prescience ero of distribular systems. That was followed by one of the most but proven insolvable. Right again, that was unsolvable. Correct, there's no school problem. The way it's posed is unsolvable. But the number of exactly that? That's very good, good point. Call it that. It is the the number of acknowledgements you require to be absolutely sure that the other party will attack at the same time as you is infinite, it turns out so. But that was that was, I think, in some ways, the first for a into what is and is not possible in terms of online coordination, and so that problem evolved in into the Byzantine General's problem, and two people really formed that area. One of them is Leslie Lamport, the other is Barbela di scope. Both of them have touring awards, very well deserved for their contributions to this area and following the framework set up by Lamport and less coupe, people came up with hundreds of protocols that are in what we call the classical domain. Their classical protocols, typically what you have is a small set of validators, small set of participants and they need to take coordinated action. And typically what you then have is they talk, everyone talks to everybody else and you reach some kind of coreum. Typically two thirds of the people have to agree and with that agreement then you can move ahead and take action. So these protocols are used. They're not used all that much actually, but they're making a resurgence. They made a resurgence with permission systems. So when you have a small industry consortium of let's say ten to twenty big shots in your industry, you know you coordinate them using something like protocols go by different names like pbft, is in the classical domain. And then you have s, the recent entrance where people go back in history and they dust up one of the old papers and you hear things like head our Hashcraft, for example, squarely in the in the classical domain. It's essentially a rehashing of PBFT, just kind of slower down, done differently and without a leader. So so you have a lot of people working on the space in the in that area of classical protocols, but they all have the same strengths and weaknesses. The strength is these protocols give you on a hundred percent guarantee you know one day say something will happen. You know they will happen every like the sufficient people have been have been contacted for them to decide. But and they do it efficiently. There's no mining right there's just a bunch of network messages and then you're done. So that's kind of Nice, but at the same time there's a whole bunch of drawbacks. These protocols don't scale if you have large numbers of participants. If you have more than a hundred participants in a hundred squared is already tenzero. That's a lot of messages. If you have more than a hundred, you know it's just balloons out. If you had a thousand participants, a million messages. That's a lot. Nobody can really reach those numbers. That's why, for example, facebook's libra coin which is in the classical domain, is planning to scale to about a hundred validators. That's why etherium to oh which is also in the going back to the classical domain, is planning to have about sixty four participants in terms of validators. That's why EOS has a small number of PARTICI. Eppens twenty one. They can't handle more. I can go on like this, but you get the point. Classical was what we had for thirty years and if you mastered that, you became an expert academic in Byzantine fault tolerance for the longest time. And then income Satoshi, and he's brilliant. He looks at all this stuff and he says, look all this crap that you guys developed, at you academics did, and he's very right about this. All the stuff you did. It's inapplicable to open systems. For them to work, everybody has to know everybody else in the system was making decisions, and that's at all order right. It's like it's okay if I have a non changing or very infrequently changing set of participants, if it's like the Senate, okay, I've a hundred representatives, I can know all of them and they can. They can reach their two third squorums and that and do their thing. That's no problem. And but if I want to have an open system where nobody tracks participation, where you know some people, I know some people, but you don't know all the people. At the same time, we definitely don't agree on when, for example, Corey entered the system. You know, for me he didn't yet and enter the system and for you he did. Those the classical systems all break down under that model. So so. So she looks at this and says, I'm going to come up with a different scheme, and he comes up with the the very elegant use of proof of work for consensus, and and that's that's his contribution. Of course. The downside with that is it doesn't scale. It's got these blocks. The blocks can't be too, too large. They can be largeish but they can't be really large, but they can't be too frequent. So you're limited in the numbers of transactions you can clear per second and and there's enormous amounts of energy being used in the background. These are probabilistic protocols, by the way. They don't give you a hundred percent guarantee. They give you a ninety nine point nine nine, nine nine nine percent guarantee. And it turns out that in theory these two things are totally different. Right me saying something with a hundred percent probability versus hundred minus Epsalon they keep. To a mathematician, these are different numbers at all, all together, different ballgames, but in in practice they're the same. There's no difference between, you know, a whole lot of nines and and a hundred percent. Why? Because, with some probability, my CPU will miscalculate. Okay, the machines we use are probabilistic devices. With some probability, some Alpha Particle will hit my CPU and I will think I'm doing something, but I'll be doing the wrong thing. I will have miscomputed. I feel like you're trying to cut off one of your criticisms at the past with this, this, this line of thinking here. No, no, I was trying to give some credit to Sutoshi. But that's true too. There there's people can want to lodge this as a criticism of avalanche. I will gladly take it on, but but there really is no difference between these two numbers. So it was fantastic and it was brilliantly. The problem is these protocol goes. They can't stop. There's no termination for bitcoin. The miners can't just turn off their machinery. If they had time to do that, they would have time to go back and rewrite history. So they need to be constantly, constantly incentivized to participate in the system and they constantly consume enormous sums of energy. In this, you know, game of one upmanship. If Corey adds, you know, a couple more miners and you want to preserve your mining hash rate, then you have to add minors to and of course I will feel compelled to add some, some more miners myself, and you have this crazy, crazy jockeying for position that you see in Bitcoin. All together it ends up consuming enormous sums of energy, about as much as for nuclear power plants. That's a lot of energy. All Right, I'm a little longer than today, but let me just sum it up. So we have these two approaches in about forty five years of research. One is classical, the others not Comoto, and both are brilliant. Both are getting that the same problem by different routes and they have very different cons and pros. And then incomes avalanche and it combines the best of Nakamoto with the best of classical. So it's probabilistic like Nakamoto. It's Lucy Goosey like Raco Nakamoto. Or the other way to put it is it's robust. It can handle some difference in terms a Gouzy Lucy Gussie. I just made it up. It's an I want to see it in print, in in an academic journal. Work on this, but the bottom line is it's very robust, like you and I, and you know, like the part of supposed to the system. Don't have to all agree on loose in the system and it still works just fine. And and it's very efficient. It has quick finality, like classical, so the system can cut come back to you in a second or so with finality that's faster than my my credit card, by the way. It's high throughput, so it can do tens of thousands of transactions per second. It's I just did a demo, by the way, in May, and you know, the demo ended up getting six thousand TPS. So people were like, you know, it's constantly saying, oh, we could never reach Mesa level. Blah, blah, Blah's like I don't know what you're talking about, but I have a thousand validators doing six thousand TPS and I didn't even try. Like this is how a thin context, the topology that network, makes a difference in terms out for through point I'd said, and actually that's one of the questions that I'd get it to later, is like how those demos were made and what that means relative to a realistic scenario of an open up of an open network geographically dispersed. Right, if we can. For now it's like I definitely want to hear like what are the features of avalanche? Like I have this this article that was that was written on it. It's actually who wrote this. I don't even decide to check. It's by Fox. Anyway. They got this nice chart and they show like all these features comparing Nakamoto and classroom the avalanche. Right now, avalanche has all the check boses. So you're just like, okay, when I see that, I go okay, what are leaving off here? Yeah, so it's listed as like Nakamoto's robust classicals, not avalanches. Okay, avalanche is highly scalable, quick finale, the high throughput, lightweight, sustainable, green and qut question. It is easy to implement and understand, although I got to say I'm still having some understanding issues still with avalanche. But I think that that that will sort itself out once I see it in it's in time and then safe in the presence of fifty one percent of tax. What I don't see on here's things like, you know, storage requirements or anything like that, and so you know, I'd really like to hear some of the features of avalanche and I have they compared before we get into too much. What coury just said sounds good. Let's do all that. Let me also talk to you about one critical thing, which is the difference between avalanche and Ava. Avalanche is the core protocol and I think we're beginning to see avalanche adopted into different settings. Like vch has an implementation. Says those as an implementation they're working on, just as I predicted when the announcement was first made. The the the core protocol is so, so cool that it's going to find this way into multiple coins. The particular coin I'm building is called Alva, and Alva has its own unique model that's very different from everybody else who came before us. So we're innovating not only at the the protocol layer but also at the layers of Um. So super happy to tackle all of those those questions. Super Happy to talk about the core protocol itself. It's weakness is, its issues and so forth. So so you lead the CONVO. I will try to do my best for likely shortens. I'll start with I'll start with let's let's continue with avalanche, because that's the more, I'd say, broad approach is to stick with them where general idea of a new consensus model and how it works and where it may be applied. Like our we're situations where it should be applied based on its strengths. Right, and we can move into how ava, the actual blockchain network, implements it and how, and like it's architecture and how it takes advantage of the type of things. Great, let's do it. Okay, starting out, what is it? What's a drawback of avalanche? What's a drawback on that left little before I answer it, I have to give you my typical my typical response to this, which is, you know, people don't believe you when you have a better thing. Okay, I won't. They they always think there's a catch and they've been they've been trained to think that every distributed systems, every distributed system and bodies a fundamental trade off, and that's just not true. So, you know, it's like, so what's the PLA? You know, it's like somebody has a shovel and you come in with a steam shovel. Steam Shovel is just going to be better at everything that the shovel does. Okay, or, you know, take a better example. Yeah, but the cost is a supply chain related to gas and fuel like. There's there's almost always a tradeoff to everything I do. I don't necessarily buy that line of reasoning. I'll give you another one called and how about about burritos versus Tacos? Okay, clearly the buried so a superior technology. The stuff doesn't spill out the other end. I like cake versus pine. Okay, you get the idea. It is possible for something to be strictly better on on all known or all relevant issues and maybe slightly different in other aspects. So so, with that out of the way, it's sort of my my joke answer, but it is an answer and it has to be said that things can be better across the board, but let me try to identify some weaknesses for avalanche. So let's see. Avalanche doesn't work well in small settings. If you want to have a protocol for a small number of validators, if all you've got our twenty seven validators, like facebook libra, and they all paid ten million dollars to to be there and they're going to do facebook libra things, be my guest. Go use whatever you want to use from the classical domain. And in fact they they announced that they're using a protocol called hot stuff, which was done by my student, Ted Ted Dan at them where with mentors they're and Ted is currently work on avalanche. So they're fine. You know, it's okay, go back and use that stuff. It's fine. So that was a very subtle middle finger. I love that. Not That I love them. I love the facebook guys and they're doing they're doing. I'd say they did their homework. If I look, if you look at the White Paper in attecting white paper, it's not it's not hard, well, hastily thought about. Yeah, non, bye bye. By all means. They did their homework and they picked the best st of classical. I think hot stuff is the cutting edge on that front. But okay, so number one is for small and small networks. It just doesn't work, though. You know, avalanche is you'll get nothing, no goodness out of the avalanche mode of operation. Other other issues with avalanche. Avalanche is a new protocol. There are lots of subtleties with parameter selection for avalanche and there's lots of degrees of freedom. I have seen many other efforts to implement avalanche and when it comes to parameter selection it suddenly gets difficult for most people, most people. So the work that the designer has to do is is is harder because it's just very brand new. And we ourselves find that, you know, find ourselves sort of at the white board thinkings issues through, trying to pick reasonable parameters. And so I Yota, for example, is adopted or is trying to adopt a variant of avalanche. They have decided to take one number that that was static and they want to make that dynamic and we'll see what they they managed to do with that. I think they will find that picking these numbers is not it's not it's not that easy. So those are the two big issues that I see the guarantee it provides as probabilistic. So if you want a hundred percent, if you want that that for you yourself, you're not going to get that out of avalanche. I argued about why. You know, probabilistic versus a hundred percent determine as to guarantee doesn't really make much of a difference and in fact it was part of the brilliance of subsociates to note that those two numbers, that hundred mindcept salon equals it, equals equals a hundred essentially. Yeah, but that thing with like Nakamatic Innsensus, if it works appropriately, is that there's an economical change associated with that probability. So it becomes infeasible after a certain amount of time economically in the real world to change that type of thing. So there's like a there's like a realism associated with that probability. This avalanche have something similar to it. What's the realism associate? So that's assuming a whole bunch of things about the value of the coins. The difficulty adjustment is definitely a lot of that. All the Hash rate and the associated energy going into that Hash rate and the value of of coins associated with it has a lot to do with the economic feasibility of rolling back that block, rolling back the blockchain improve. But to be real, the I mean like the coin, would have to be sub a dollar to even, like, not after like to revert like fifty block you know, I mean like it's something that you gotta, you know, take it too account that it just multiplies on itself as the more more progressive, so it reaches an eventual probabilistic, you know, practicality as I think. What I meant is that what you're trying to get at court. Yeah, but it has, it has, it has, it's that probability is based on real world valuable resources. Okay, so that's so's what you said is exactly right. That's true, and avalanche doesn't really say anything at all about this particular use of of the protocol. But the way we're using and using it in avalanche, in Alva, is one where there is a similar economic argument, which has to do with the money the validators are putting into the system as part of their steak. So what you done have to do to revert is essentially give up on your stake. It would end up no nullifying the the bond that you put into the system. Just like in Bitcoin and other power currencies, the miners tend to lose a lot, as you point out right so they could do all sorts of attacks. We never see attacks against the big coins. We very, very rarely encounter these. We do see them against the smaller coins, that's true. But but the big ones haven't really been attack because there's so much at stake. The people who went into it, the miners, they put so much money together that they'd be fools to lose it. And when I say money at stake, I I mean that that got the pond, is intended that there's their their proof of stake systems. It's just EXTRINGC statement, exactly right. The the rigs are the stake, right, and so they put in a lot of money and they don't want to see that money burn and still profle stake systems work exactly the same way. There is a slight difference between intrinsic versus extrinsic stake, which has to do with the following. In proof of work systems, the mining rigs, which are the stake that the miners put up, can be used to target a coin and after the attack, a successful you can take the rig and mistake and use it elsewhere. So that's why you've seen people try to attack bch. You've seen people do things to bitcoin gold and I think bitcoin private perhaps as well. We've seen the annults many, many times. People attack, they they crash a coin, then they take the rigs of elsewhere. So in proof of steake systems they have much greater security for the amount of dollars invested because if you attack, your steak is gone. If you crash the coin, whatever you put into it to launcher attack is no longer yours. It's dead. It's zero now. So a successful attack really kills off all of the the stake of the attacker, and that's not true for proof of work systems. That's definitely true. Sorry, and taking that a bit further, what is what is avalanche really really, really good at? The think that of a context of a distributed system, coming to a decision on a piece of information, not necessarily blockchain, just consensus. Short, avalanche's main features are on the performance and inclusion front. So what is that very very quick finality about? A second very high throughput. The sustainability is a nice bonus in all of this. But I think one of the most important aspects of avalanche has to do with the ability to incorporate tens of thousands to millions of participants. US could become the Ghun rules of avalanche. You guys want to part, to see bait, you can participate. You want to have a say in every decision. You could have a say in every decision, and that's just now true for Bitcoin. You will have a say in Bitcoin. You're going to you're going to need to pull the up serious cache at this point. It's been ten years. Nobody's an early adopt anymore. It's all late stuff. So so I valanche is very different than the retort. That's because there's I think, if I had to say this on my own words, for two reasons. One and proof of work systems you're competing against others to win a game for the chance to have the say of the entire network. So what the current state of the entire network is? In a system like avalanche, you're not competing for the say of the entire avalanche, but a subset of that. Of that state is that, and so adding, adding more and more nodes or validators validating transactions. Is it competing against the same set of things to come together on? It's just expanding the subsets. So that way you're, I guess, the the throughput should scale with a number validators set correct. It's everything is correct. Maybe accept the last sense still. Let's let me. Let me try to put it in different words. So you're exactly right that proof of work systems are based on competition and people are competing to become the leader and and define for others what just happened. I want to just get the conscience say, Yo, guys, I just came up with a block and this is what I'm dictating. Happened in the last block into the last globally. Yeah, globally, I'm telling everybody. In contrast, avalanche is a cooperative game. It's in your best interest to play Nice. And if you play nice and everybody will play Nice, will then will then come to this suit. Some number of people might do their Byzantine things and we'll be able to tolerate whatever designed in number that we want to tolerate. The throughput of the system is not does not grow necessarily, at least the way it's designed right now. To through put of Alva, the dators. So you broke up. They are just the time of PA. That just me. I know I wasn't just here. Okay, the through put of Ava is what does not grow with the number of validator. It's just about no system, consensus system, will grow with the addition of validators. The latency is proportional to the log of the validators in the system. The more you have, you know, you add tenfold more than you will end up. We having to wait for one more round trip and that's a that's a factor of gossip networks, right. That's a factor of gossip and anything else like gossip, anything else that involves that kind of dissemination. So latency will go up with number of validators and I'd have to think about the effect on throughput. It would not necessary. It would not necessarily go up. I can tell you that much. It might have. We did all lots of clever things at the higher levels, like charding or division and so forth. What we don't do that in love at the moment. That now you say that was a log ten functional latency, or did I misunderstand that? I'm sorry, I was kind of have trying to check something else. Out like you said that, did you? It's like if we had ten, you know, a factor of ten more validators, yes, and that would increase the latency by by one, by one. Okay, it's like log right. So and ful tenfold adds one to your latency. So another tens, you add hundred, hundred x more validators. You get two more round trips out of it to your latency and so forth. You get the idea. Okay, it's I don't know. It seems to me as though that a lot that this came from the research in like peer to peer reputation systems. Like there's a lot of there's a lot of similarities here, in my opinion, are not. HMM. Well, if you think so. I worked on peer to be a reputation I it's you know, maybe it was in the back of my mind, but you know, I don't know where it came from. I don't know that these ideas were seen in the reputation systems before, but you know, you tell me. I see it as like, I don't know, a lot of those have components of kind of local consensus as well as, gossips, global consensus, and in a way to combine those two things into a single answer that everyone should come to eventually, and so you pack with avalanche like, if I were to say, like the very, very small, compact way in which avalanche works. You have a decision to make, what I think the standard one is the color of red versus blue, and in order to do that I have an answer, but I would like to see what the network thinks. So I randomly choose a subset of my peers and ask them that question, I get that answer back and then take the majority answer. I do this multiple times and the process of doing this eventually you come to everyone has the same answer, and that's the that's the gist of it in terms of avalanche. My my wrong there. You are absolutely cool. That is exactly how I have latch works. You were based off this for some number of rounds, at the end of which all correct knows, will have will be holding the same color in their hands. And my mistaken am like, based off what you just said and based off basic cord just said, and how the alt to reach the eight to be like, let's just say, assuming etheriums, fourteen second block times to be one second worst than the theoryum we need ten trillion you'd have to have ten trillion validators on the network and that would be creased latency the thirteen set. So I cause assuming that it's kind of in that range. Is that approximately correct? My mistaking something about what you're saying? You've something like that? Maybe. Yeah, I mean you wouldn't want to go even if, even if you're removed that by two factors of ten, it's still are you three? That's still pretty a lot of validators. It's basically IOT level support for IOT enabled smart city kind of world. Oh Yeah, Oh yeah, the most, definitely, most definitely. So, yeah, no, as I mentioned before, we were getting six thousand tres actions per second out of a thousand validators with sub one second latencies. So, yeah, there's a lot of room to grow here. You want to have a gazillion node for some definition of GAZILLION. That's really large. Avalanche is the protocol for you. None of these other things will scale if based on based on what I just said, that's coming to the Christians, the decision on a single piece of information. I didn't do that. When you have a global network submitting multiple, multiple transactions, that throughput has to grow relatively quickly in terms of what a single machine can do. So as there a resource limitation to a single validator? Yeah, well, typically these protocols will be limited by the bandwidth of the validators themselves. So there's some kind of a bisection bandwidth to any network. Right. So you have some numbers, numbers of nodes. Think of it as a Pie, and then you get get to cut it any which way you like and you take the average of the of the cut. Okay, so to take your network, you cut it, you look at the bandwidth between the left side and the right side and you do it a bunch of times. There's going to be some average that's going to be a defining limit for what that network is capable of. If your network is a whole bunch of yogurt containers and strings, then your bisection bandwidth is going to be very low and you know you're not going to be able to get much through. But if they're all well connected, then you'll get great group. But so that's that's a defining factor regardless of what protocol you use. The funny thing is we don't know what the network is capable of. It depends in emergent thing. It changes. It changes depending on what the backbone fiber providers actually put in place. Right it changes based on what people have at their homes, what the cable companies doing, so forth. It changes as you get more nodes, depending on where the nodes are station. So so we don't know and we'd love to have protocols that are capable of splurging and taking up the slack of whatever is available and then then then being able to take advantage of all of it. Now, protocols like mackamoto, like Bitcoin and etherium, one hundred and two, you know, as it is now, they can't actually go to the Max because to do so they would have to make a guess as to what is the bisection ban with and and back off of it. This was at the heart of the block size debate, and the block size debate wasn't a whole bunch of budget. There was a lot of bullshit that God said. That's true, but there is a core tradeoff there. So you can't really push your parameters all the way to to what the network is capable of because you don't know what those are. And if you just willing nearly up this counter the block size, at some point you will end up cutting people off. So you need a protocol that naturally and gracefully finds whatever that thing is and operates at that level. And Avalanche is sort of gossip based operation, does exactly that. It figures out how fast the notes are going and only makes progress when sufficient responses are received and and therefore it manages to find whatever is available in the network and take advantage of it. So something I'm kind of unclear on at this point still is in most of these like Nacomatic, a sensor systems, is a kind of sort of confirmation and and it seems like your whole network is one big confirmation, like and so I'm trying to figure out, like what does finality mean in terms of like Ava and avalanche and and how is it different than Nakamoto? So I have to wait for how do I know when things are finalized? Does that make sense? Great Question. Yeah, no, that's a great question. Let me make it clear. This is a very simple answer to this. So, as you point out, in Bitcoin there's a blockchain and you have transaction in the last block. Of course you can't trust it because somebody else could come up with a new block and then the White Paper Sot she does the numbers and you know, and computes and says, Hey, look, six blocks is good. If they as long as the attacker isn't bigger than thirty percent. Six blocks gives you a very good, very good guarantee. So you wait for your turns actual block to be your transaction to be buried under six blocks. So that's Bitcoin. There's analogous situation in avalanche, and avalanche we're not building a single blockchain, a linear chain, we're building a graph. What's called it directed a sicklic glass graph, a dack. So there's this dag being built and you want your transaction isn't final until some number of Poles have been done on it, and that number of Poles has to do with how deep this, this transaction is in the Dag. So what do I mean by that? Imagine that on day one we have the genesis VERTEX and people start attaching some transactions to it and then other people attach other transactions onto those transactions and so forth. So there was some depth to this Dag. There's the early stuff that's really, really old, and then there's the latest stuff up until what we call the live edge. So I'm trying to sort of draw a picture in the viewers minds. It's a little tough to do, but you can imagine that there's this live edge that hasn't yet been decided and there's the stuff behind it that that is deep enough. Those deep things are confirmed, they are finalized, they are essentially impossible to revert because the network has spoken and they pick those transactions in contrast with others that might conflict with them. If you do do something, anyone that say I look, you know this thing in the past did not happen. Instead the money one somewhere else, the network will say no, no, no, that stuff is really deeply buried in the DAG. We already decided and your new transaction it's unconfirmed and we'll net, it will be finalized, rejected, it's out. So there's a very analogous burial sort of scenario playing out an avalanche as well. So you have but I could if you try and look at how other gag like systems and blockchain work like we're good, look at iota. The way that they come to consensus or or, I guess, increase confirmation on transactions is that every new transaction has to confirm to previous transactions or something right which hads downside. I mean, basically, you have to have throughput to fuel the system, and if there's no a sufficient amount of increasing sustained through put, then the system starts to backlog. I wanted. I wanted to put forth, though. Sorry, core in its general concept, in that that the blockchain is just a data structure. But you specifically choose the architecture of these block chains so that your can sense spechanism works appropriately. Like you can't do knockamoto consensus and, as you have I a succinct Hash of our fingerprint, of the entire stay of the network. You can pass around. What are you doing? An Ava with the DAG that allows you to take advantage of avalanche. So okay, so exactly what you said is right about iota and in avalanche were doing so I ought to you know, the consensus protocol. You're being very generous to them. Their consensus protocol is incorrect as it is, and the new one is actually generous. But if there's value in being generous, I like those guys and they're fine. You know, I wish them the best with with their new protocol. Let's see. So in a in Alva what we're doing is essentially introducing new transactions that confirmed previous ones, because every transaction is going to record its own polling and it makes sense for everyone to piggyback on other other Poles. If I'm going to be doing some pulling for a transaction anyway, why not allowed the pulling to confirm previously unconfirmed stuff? So in Noma you can have some number of parents. I think the exact number is up to you to decide, either one to five. You don't. We don't want to have two too many because, you know, just for storage reasons. But some number of parents are up to your discretion to to add to your transactions so that you know you help others go through the system. If there is no troop but in the system and there won't be a backlog, what instead we will have is a bunch of repeat of the polls to try to push those backlog transactions through. You don't need to. You don't need some of these genuine transactions to push old to push the old stuff through itself. You just need to repeat, repeatedly pull the network some number of times. If something is sitting in your que unconfirmed, you just pull and pull and pull until you decide okay, this is confirmed, and how many like pollses it take to to confirm something? And who are you pulling from? Like, how is this that determine it? Does that matter? See to me it's like he's like, this is like differentiating basically iota from from avalanche, in that they're both that based systems, or aabi should say, because avalanche is really the consensus mechanism. We're talking about the day, which is part of the avos to ecosystem. Technically you could probably use different data structures to represent this, and I think you mentioned that on the last show. Right come. So differentiating this, I mean I know the problems with iota and that you know, the throughput issue became huge where you'd have at seven minute eb seven hour confrogation times, but right when they first launched. So like I'm trying to figure out what amount of polling, who are you pulling from? How is that determined? Does it matter? How are you finding people to pull that right thing it? It's all matters. These are all important questions. I will very quickly answer all of them. You want to pick people at random and you want to discover as many people in the network as possible and we have protocols for doing all of these. These are fairly well understood, simple processes and, having established some notion of WHO's in the network, you want to pull them at your own discretion. You pick, pick the ones that you like and you ask five to ten people. How many times is the crucial number? And that depends on how big you expect your adversary to be, and for realistic attack scenarios and realistic numbers, the answer is somewhere between fifteen to twenty. Five rounds. That's a tiny number of rounds. You're in a very littlest sorry, and I should mention for the network of size tenzero, two, a hundredzero or so, and that's three people are you pulling for each round? Five to ten, okay, that's what so five to very small number. So as a notice, this that stadium example you gave us offt time. Basically. Okay, I'm starting to actually pieces together now because, like you know, when you give it last times very early. The team rocket paper. It recently just come out. You know, we're still involved in all these other kind of get set suspectives that I'm trying to catch up on as well, this point I think I kind of get a I'm starting to see where you're coming from with this. So, basically, take the iota tangle and you add this pulling mechanism on to it. I probably shouldn't say it like that. All right, I know that's angle. Dude, very well, whatever, coordinator, I don't the apologize. I know I saw I saw you cringe when I said that. The audience can't see it, but they could verbally hear it. But you know, the point is I'm trying to relate this to things I already knew that didn't work, how you might be approving upon them. That kind of thing right, and that it's important for me, as a foundational to understand the bigger picture, to build off of what I've learned and then, basically, like be that the Costello to cory's Abbot and and and you know, dummy, my way through this a kind of understand things in a very basic way, and the way I understand it right now is through that previous dack work, such as, but not limited to, iota, where, you know, they had this issue with a three put but basically didn't have this polling mechanism. They didn't have the insight that you did, and so what I'm trying to isolate is that insight and and the mathematics behind it, especially as something I have to kind of intuitively feel. So maybe you know calling. There's one thing that's I can say that's really helps people, which is imagine you're trying to decide, and then that's what you're in a giant stadium and everyone's trying to decide between red and blue and they've gone through the process and and so you can see that if everybody has selected red, there was no reverting that. Right. You come in with blue in your hand. You ask people are Oh God, no, no, no, like we be picked read. But that's easy. Now go back a step. Imagine you were the last guy with blue in your hand and everybody else is read. In the next round you're guaranteed to be read right and go back one step. And so you know you can go backwards like that. You know at some point sufficiently far away, there's it's put. It's possible for you to veer back towards blue, but it's negligibly small and if it's open, it upon your malicious actors, the guys refusing to turn round. Yeah, that depends on the exactly the Byzantine, the size of the busintine component. So so you can see how the intuition works is that if there is a sufficient big, sufficiently big crowd that has made a decision, then then there is no reverting that. That process will just carry itself through the completion and it's a very powerful process. It's the reason why the UASF succeeded by as well, by the way. So the user, user, user, initiated soft fork. If the entire network says something and you want to come in and say something different, you'll find the the rest of society pushing back against you and the people you want to trade with are in the different reality that you have to conform to. So so anyway, it's not sort of the rough intuition that that's behind the avalanche protocol. It's just US astually want the end to move the masses to a decision, to a single decision, and this process is proven to terminate very quickly and, as I mentioned, the number of rounds is very, very small. So for you know, tens of thousands of nodes, you do it for whatever, like fifteen to twenty five. This is a twenty rounds or so you're contacting ten people. Twenty rounds. It's two hundred packets. It's nothing, and Iot device could do that. So I remember when you gave your consensus talk last year and when you talk to us about this last year, you mentioned that there be a smart country, smart contracting laments on top of this. How does that work in a situation like this? Great Question. Let me talk a little bit about the Alva model. So when you extended to avalanches, is the protocol from team rocket, right. So it does all this stuff and you ask me about it strengths. I said, Oh yeah, performance and and inclusion, the ability to allow people to join. But that's not it. For Ava, AVA is actually innovating across the stack and I want to talk to you guys about it in the remaining time. There isn't all that much of it. So one of the things we added on top is the ability to have multiple coins, the ability to have multiple scripting languages and the ability to have multiple kinds of nodes. And I want to expand on this in a bit maybe, but let me answer your particular question. So we added the ability to have smart contracts to Ava. Our smart contracts are going to be written in evm or potentially also wasn't, and we can support multiple, multiple multiple scripting languages, and so at the consensus level, I mentioned that we're computing a dag and you are probably going to say, because you guys are very savvy, you're going to say, Hey, smart contracts typically require a totally or or their timeline. You want to know that x happened, then, why happened, and Ze Happen? You know, you're selling tickets to a concert. How many tickets are outstanding? That call in come first, or did corey come first for the last seat at the conference? We have to know these things and the DAG is not exactly amenable to this. So in Ava we provide totally ordered timelines for smart contracts. So, if you think about it, a blockchain, a line right, is a link list is essentially a subset of a Dag. Every every link list, every blockchain is trivially also a bag right. So so we can embed any number of any number of totally ordered timelines inside the Dag. The Nice thing, of course, is they're independent of each other. So if you have a smart contract and I have a smart contract, yours can get extended at the same time as mine without interfering with each other. If your crypto kidneys, and I'm a DEX, I am unaffected by the activity happening on your timeline versus mine. So that's kind of a nice feature of of Alva. We can have high throughput parallism as well as a totally ordered timeline within the context of attack. How state managed for that? Once you have this totally ordered timeline, it's managed in exactly the same way as it is an etherium. So there's a state route, every update changes, the state modifies it and there is a new hash summarizing the the latest the latest state. Something I read, something I said regarding how transactions are sent or for coming on to the consists for avalanche, is that you need the entire history of a given state in order to come to come to consensus on it. Is that that is going to have an issue. I sorry. First, is that true? Like, no, it's not. I'm not sure where you're read it again. Just I just misinterpreted it's a I guess it's Chits. Yeah, the color of the concept of shits applies to your progeny, to your children, not to your ancestors. So I don't have to know your ancestors. You know if you're if you're showing up at my doorstep with, you know, tenzero people who say you're awesome. You know, I don't need to know what's happened to your parents, right, I don't. I don't care about them. It's similar way in which, like, basically, if I were to make the analogy of how, like, say, a theorem works, right, Theorem is a similar type thing. It's just the DAG is just a single line, that's right, and you have to fit all smart contracts and all state changes into that single line. Right. It's very easy to reference. But in the concept of a DAG, or at least a more generalized Dag in which you can have multiple different paths, branches and so on and so forth, a smart contract is its own line, right, and then if you would like to use smarty smart contractor smart contract talk, you have just links between those two different lines. So it's more like concurrency than everything flowing through a single pipe. Exactly right, exactly right, and that's one of the reason is why we get high performance that's right exactly, as well as being able to do multiple scripting languages and so on and so forth, because they're all basically independent of each other. Yeah, there's I mean that's something slightly different from the rest. It doesn't have anything to do with the structure of the Dag but but yeah, so that's something we can do by virtue of the way we constructed the rest of all. So that point really requires some thought and some some discussion. So maybe I like kind of want to mention that. It's important. So everybody else in the scryptocurrency game, everybody like you, open up coin market CAP, one thousand eight hundred tokens. All of them have the following simple mindset that they all stole from Satoshi. And I'll tell you the mindset. I have a coin, I have a scripting language and I have a network. One coin, one language, one network. You want to make a change to something about how the coin operates, well, you have to go talk to those network operators. You want to make a change the Bitcoin, you're gonna have to talk to Luke Jr and he's going to tell you that he's got some weird, very far swamps of Florida and he doesn't Raspberry Pie can't keep up with it, because that's the universe they live in. And and he's right, like there is no put. You have to get him to be on board, because there is one network like that's that's why those guys fight so much. So, you know, should the operaturn be fourteen bites? You know he doesn't want it to be eighteen bites because but that. Now we can go into this. But they have to have all of those rights because they have their fighting over the same single platform. Now you look at any other coin, you name it, I ought and them neo whatever, like all these latecomers ethereum. They even though they came late, they were unable to break out of that mentality. They just copied what'Suto she had. Ava Is Different. We're not doing that. We imagine a world with millions of coins. Every single stock certificate is a coin. That's how I view it. Every single real estate parcel is a coin. That's how I view it. There will because Zillions of these things. Then there will be scripting languages that apply to them. So you know, there's the Bitcoin scripting language that's supported by wallets. There's the EVM for smart contracts. There's wasn't for new people who want to write in a different language. There might because Allians of others there. There's zero knowledge proofs from Z cash. There's ring signatures from one Arrow. There is a whole lot of different rules that apply to gold, that apply to real estate, that's applied to boats, that's apply to you know, you name it. For every kind of asset there's a different scripting language and, most importantly, a Nova. There's also a different set of nodes that could be providing different sets of services to different kinds of coins. It is not a lowest common denominator single network. It's an interoperable network with, if you will, different individual islands inside. So, for example, You Could Issue Colin Coin Tomorrow and you say something like Hey, I want, you know, the Colin coin represents land in the state of you know wherever Virginia. You can mince those guys to go with you. So it obeys this real estate rules. It you can calls two coins that are adjacent in real life, etc. And you know, you also like zekps and ring signatures. You want anonymity for these real estate transactions. But, by the way, you want you're not going to get people to store real estate for you. That's not what regular people do. So you're not going to be deploying on the default of a network which is going to have, you know, the default notes on it. You might very well say, look, the notes that support me, our knows that are colored purple. Now who's colored purple? Colin decides what's Colored Purple. Colin Collins Purple. So in ASENCE point is why not? Calling right, and it's a good color. It's a good color. So so now to get that purple color from you, they need to get a certificate from you so you can pay them out of band for providing additional services. You can say, Yo, look, you're going to be doing real estate stuff, you'd better keep these these records around for fifty years. I will pay you accordingly. And, by the way, you know what you're dealing with. This Colin coin is not like regular coins. It's not all it's something else and you get to get transaction fees. Commiserate with this service you're providing. So you're going to be storing this crap for fifty years, you'd better get you know, when I bought my house, it's was Fivezero Bucks. Right for the record keeping aspect of it, the transaction fee ought to be five thousand bucks, or maybe up to Fivezero Bucks, because it's a different kind of service if that's what it takes to have that longevity. So so this is really different for Ava compared to everybody else, these other guys, you know, they've had ten, ten years to play in their sand box and in ten years all they did was bigger fight with each other and and, at the end of the day, Form Weird tribes without actually innovating on any front. We innovated, not only obviously at the by taking the best known consensus protocol, but also by coming up with a new system model that is better grounded. It's legally better grounded, it's businesswise better grounded and it leads to systems and deployments that other people cannot even fathom. If people in the trenches fighting over inches and you just went to the air, Gotcha. Yeah, I think we're not. Like we get attacked by all sorts of people. There are lots of copycats. Are People who are claiming like Oh, you know, you know, we came before you did, whatever. It's like all sorts of fighting. We don't engage in any of that. We don't care. Like my eyes, on the three tillion dollar, three trillion dollars of assets that are not digital right now. That's what I want to go after. I'm not going to make five more bucks by taking it out of Bitcoin or whatever, not like that. Money is fine, it's it belongs where it is. I don't care about those. I'm not going to fight it out with iota. They can do whatever they want to do. Not that know they're doing all that much anyway, other than hotel. So you guys. I really don't want to don't want this to go too long for you, but I do have one outstanding question, if you do the answer it. I'm not really clear on the storage requirements when it takes to actually sync with a network. Right. So, okay. So, yeah, this was the previous question. I said no to it, but really explain. So you do not need to store the entire state of the deck. No, needs to know the current live edge plus one, so to speak. So you need to know the live edge, you need to know the confirmed transactions right behind the live edge and you need to know the entire outstanding utx so set, if you will, the equivalent of the Utso set, or everything. You need to have the you know the outstanding balance as a current state. Current state. You need to know the current state, which is what you're keeping tour about, and the current unconfirmed transactions, but you don't need to have the entire full history of the world. There will be archival notes that keep track of that, but a regular note doesn't need it. So coming up the speed should be should be easy. Now storage requirements will depend on how fast were we run the network, and the Nice thing about the model I just mentioned is it gives you freedom to join the different kinds of groups on the network. You want to join the default network, you're going to go at the speed of the default default system. We foresee that that speed is going to be high. I wanted to be about FIVEZERO TPS or so so so you might very well end up going. If we end up getting FIVEZERO TPS sustained, that means we're outdoing visa. We're doing amazingly well and that is the kind of success problem I want to have. You basically, black Friday every day. Black Friday every day is fantastic. Bring it on, and if that's my problem, then my users might start to complain a little bit about the storage requirements. which translates to approximately a hundred dollars worth of storage per year. So what they collect transaction fees and staking fees for this. So there is that. They also get the appreciation from the coin. So you know, it's not it's not the end of the world. But if they decide that the costs are too much compared to how much they're making, of course the network, and not I, but the network, will listen. In fact, Alva has governance built into it, so if the users want to change how it works, they get to vote and they get to to make their voices heard and they can change key parameters. Every key system parameter in AVA is up for governance. So you could say, Yo, guys, I see that we're minting coins at a rate of two percent per year. That's too much because the price is tanking for whatever reason, and so therefore we should go to one percent and and if you have enough people voting with you on this, they the system will then move to that point of operation. Or vice versa. You can say, look, we're minting at two percent. It's not enough. We need more stakers, we need more people. Let's go to, you know, three percent. So I have to say a couple more things about this governance mechanism. It's not like what you people have seen in Ta tezos. Not Everything is up to change. So things can't change drastically intenzos. There can be a vote and suddenly we're in a different universe. In Ava, only certain parameters can change and they can change by only so much per time unit. So essentially, what I want to do is I want to be able to go on vacation and not check anything, and I want money to be a calming thing. I want everybody to be able to sleep well at night knowing that they're not going to lose the coins that they put into the system. So I don't want to worry about slashing. We do not have slashing in our system. There even if your machine is hacked or misbehaves, you will not lose the the coins that you you changes that can happen that that end up end up changing these key parameters. So the minting rate, for example, can only change by so much, say around ten percent, for some time unit. So you know, you go on vacation, okay, we know the minting rate can move from two percent to two point two or down to one point eight. But but not drastically to zero percent or t twozero percent. It's not going to be insanity inducing numbers. So what time remaining, which is not a lot. I was I was as very curious as to the tests that you've recently performed that they give you these numbers. What was the like? Can you explain how these tests were done? Sure that I could. I can have a better idea or how to evaluate the results of them. Sure, I will relay them as best as I can remember them. The bottom line is as follows. They are real test from the real system, that is to say, our actual go code is running on a bunch of machines. It turns out that getting a thousand machines on a Wus is not trivial, but we ended up getting a thousand actual nodes on a Wus. So this is exactly the same as a thousand different dedicated machines scattered around the world. The the code that's running is the latest go code. It is not super optimized. It's got a bunch of different features that are not present in Nice C plus plus code. Our C plus plus code is much faster. We were getting nineteenzero TPS from our C plus plus implementation, and our go implementation was about six thousand. And the nodes are in multiple different availability zones, so they're they're scattered across the globe and there's a sum total of a thousand of them. Okay, every time was just curious fishes done on like a little like the kind of a fact. You look at what people have said in previous kind of test net scenarios where they run maybe like pro my owny doctor swarm on their own machine, right, and they say you could do so many, so many transactions. For second is like, well, this is not realistic because there's no world wire. Right. Do you want to give me a few minutes to talk about this? Because there you can have all the time you want. We're worried about your time. So I know there's a call. I have to go to the right now, putting this conversation. So I'm going to talk to you guys. So look, I've been in academic for a long time. I know how these numbers are measured and I have seen as a peer reviewer of other people's papers. I have seen every trick in the book. Let me go through them. Or viewers know exactly how to lie in all of these tests. I will give some names, maybe even I don't care. It's like we all I think. Great. So you want to make your numbers high, right, that's the name of this game and it's freaking hard. It's really, really hard, and most likely you are somebody who doesn't have a great idea. All you did was take subto she's idea, you tweaked a couple of numbers. Maybe you came up with a hybrid protocol, right you, or you to. You went and dusted off something from the s or the s. You know, we've seen everybody do this. Right, or you have something broken. We've even seen that. You've a centralize, coordinator, whatnot. You've got something crappy and now you want to make it look good. We're going to polish the proverbial tourage right here. So how do you do this? Correct number one, you report not transactions, you report transactions per second. But but you goose the system such that there's batching going on on the client side, so the client, instead of giving you a transaction, gives you a thousand transactions all at the same time, and you make one decision on the Hash of the of the thousand transactions. You do this, you've just multiplied your transactions per second by about a thousand right. So I've seen this time and time again. The trick number one, the easiest trick in the book. Second Trick in the Book Clos Server side batching. So transactions give you one transaction. Of the time. The server does nothing on these things, just sits on them until it accumulates a thousand of them. Then it makes a decision on that one hash of a thousand and then it pushes that through, while you gotta you got a thousandfold increase again and then you can say look, my clients don't do batching and this is an ingenious trick. This is it's a bit more honest than the first trick. The other the first trick is just lies. It's just flat out fraud in my book. The second one. Your latencies went up. They went up a thousandfold to be able to collect those thousand transactions. So latency goes up with these schemes. So anytime you see high throughput high latency systems, you know that. I don't need to name names. But anytime you see those two combos happened typically that's that's what's happening in the background. The third trick you get rid of overhead. So you get rid of the overhead of communication and your run all of your code on the same damn machine. You buy a very expensive box and you simulate the clients on the same better than gigabit speed or Infitti band speed. It's not even Gig a bit. I have even seen people do they cut out the entire network just it's just so ingenuous, or disingenuous I should say so. So this is this is just flat out lying and any anybody who does this is doing it drawn next trick is cutting out the database. So everybody and his brother is using level DB in this game. Right, they have. There's a database on the background. You have to make a record, you have to make a chicken scratch mark on this. Well, making those chicken scratch marks on disc is hard work, right. It turns out you have to write somewhere and you have to be able to find it later, and so that requires writing to multiple places on disc. That takes time, and so if you cut that you're going to get a lot of speed. And so you've got these people reporting numbers where they're supposedly building a blockchain, the supposedly keeping records and no discs are involved. Yeah, right, I guess. You say, you just running it all in memory. Agains. Huh, it's they're just running it in memory. I think it's you've seen the time and time again and it's pretty insane. Other kinds of overhead. I have seen, for example, algorithms, reported numbers, all involved absolutely zero overhead spent on a signature check. So it's a cryptocurrency where transactions come in and the time spent on checking signatures is not being reported. I think that's not nice. I think that so anytime people start to pull these tricks and other people have to pull the same tricks because the sort of the bar gets gets raised higher and higher. All right, dude, what tricks are you pulling? Man? So what yours are people in know, we're just running this code straight up now, you can here you go. Look is this is not the same. I'll be very honest about the limitations of our you know, like when we actually deploy, which hopefully isn't going to be too too long from now. You know, the numbers that we get from the real network are may not match what we were getting in the test net. That's that's possible and if not, I'll tell you the reasons why. We are still on Amazon. Amazon to Amazon connections are over fat pipes and so we're taking advantage of those fat pipes. If, if people end up deploying our network, our code, on networks that are very, very slow, we're not going to be able to reach those numbers. The benchmark we have only has a handful of availability zones. In reality, every you know, in actual deployment, everybody will be at their homes, I hope, and will be participating from home, maybe from hosting centers, but from many, many, many of them. So there might well be, you know, differences that between achievable and actually achieved. When you say builds and you're talking are they are US North America based? Have you gone to like you know any of the Korean or Germany like our? Are they all just United States based, like in your tests right now? I believe they're not all US space. We have nose deployed in all of their availability zones. Thinking that South America, I believe, for Cosset reasons we cut it out. Or maybe accept Australia, or maybe it could well be minus Australia, and maybe it's southern hemisphere is not well represented in a bench. But but you know, I don't really remember. I wasn't there for the measure that I was actually out of town. With the point is it multi continent. That's all. That's like the bare minimum, I think for a solid, solid testing. I can continent this. I mean you're only running a tobf, so you're not running anything as you're or Google cloud or anything. No, that's just extra efforts and hassle for us at this okay, I deployed. Okay, we'll call the test good and you've given us a, m'd say, a reasonable idea on how it was done so people could make reasonable expectations or conclusions on it. But, like my question is in a real in a in a in a real life scenario, when this neck or is live in people's homes, like you just said, what's the bottleneck of consensus for bandwidth? Is it is the lowest common at lowest common denominator in bandwidth? No, no, it's it's I think the it's the bisection bandwidth of the network, which is you cut the network every which way. You take the average that's going to be the main person of the think about that a little more so, but it's a bit more subtle than that. You know, we can talk about the the CDF of network speeds, of distribution of network speeds, and there is there. So anybody WHO's way too slow, you could ignore them in that sense and count them as part of the Byzantine Component. Ok, I got it. That makes that makes per sense because I guess you're randomly sampling. If you're not getting an answer back or somebody to say well, Scor offil find someone else. Yeah, they'll just find up pulling anyway and get their own decision. If they don't agreeable, then they're off the network. So who cares? Right, yeah, exactly, exactly. Okay, I think it's a great way to wrap up. Is there any questions that you wish we would have asked you that we didn't get around to? Um, not really. I think we touched upon we spent a lot of our time on avalanche. So it's it makes sense to do so because it's maturing now and it's taking form. So I hope I got the point across that there is much more going on with Ava than with just at the avalanche layer. So the I'm sorry we didn't get into that, a little deeper than I think you have. Good if we can have you back on very soon. I'm super happy to be back. I'm super happy to have Kevin be back, one of my architects, or Ted, one of my other architects, achieve for the call architects. But so that model, the change and model, is exciting. We also have a whole bunch of new ideas in store at the user experience, user interface layer, and you know, I'm really excited and and also there was this position in question that I may be alluded to but didn't really come up, which is really key for a lot of these systems. We are not we don't see ourselves as competing with the other coins. We're sort of trying to be above the fray and we are trying to go after the assets that are currently nondigital. So our our interest area is really everything that's not in the digital domain yet and we want to give people a digitization platform that can handle the demands of stocks and and everything else. I could really demanding space and other people can't really play much of a role in here. You're not going to be doing bitcoin for this. You're not going to be doing a theory and for this at the moment, and so on. And maybe one last last thing has to do with where we're going to go as a team. So we are currently super busy with defining Ava, the platform, and there are many teams in the space. They create a company that's typically a shell company. The company is useful for as long as the main net launch and then the company is disbanded and then they disappear. We do not see ourselves doing that. Of course we will have absolutely zero control over the network once main net is out right, so that is clear. But we will then continue to build vertically up the stack. So if I take a step back and I look at the space, I don't really see that many people actually developing tech. So the criticism that some people lobby at US or law at us is right that this is not there's some innovation, but it happened about ten years ago. There's a lot of copycatting that's going on right now, but but there aren't that many people with brand new ideas and I'm not really seeing the kind of development I want to see. There are lots of exciting applications of blockchain technology. I am not seeing anything that I would consider a credible push in critical areas. So I don't want to name these because they are our secret source that we're going to focus on. But just take a look at finance, like who is really tackling the high, the hard questions at the layer above? It seems like we all turned inwards. CRYPTO TWITTER IS INSANELY IN MOL oriented, it's insanely tribal and it's insanely pointless. It's not going anywhere. There work fighting over the how to divvy up a single pie. And I want to work on next, on all these things that I know are possible when you have a good foundation that other people are not working on. And I want to actually disrupt finance. I want to go that's why you're in New York now. Huh? Yeah, exactly. We're going down to what he was burker, and I want to disrupt finance and I want to teach these guys. Look, you guys Wall Street, you're doing something and it's based on a lot of manual checking, audits and human intensive tasks and we can come up with a better foundation. That just prohibits malfeasance and puts us on a much better foundation. And this ought to be the foundation for the new world. And it's not restricted to finance. It's the same problem for finances, is for supply chain as it is for medical look medical hippo records, anything like these are all the same classification of problems. He solved in one space. Exactly. You solve a roll of them exactly. All right. How do people? How do people reach you? How do they get a hold of you? Colin has one more thing after you answer no, no, we're good. Okay, I just I'm telling them and hang on until after the call, after we wrap up. Yeah, okay. So you can follow me on twitter. It's elite tax or l three hundred and thirty H F Xoer. And let's see, you can follow my blog. I can to good Acom and if anybody wants to get in touch with us at AVA, please email contact at AVA labs dot org. So that's contact at all labs that Org. I will receive it, as well as a couple of way up by people and we will respond to any and every everyone who has anything of interest or any question that that's bothering them in particular. We're very excited to work with people who have demanding needs from blockchains and want to deploy cool applications on top. You want to help you. We will not compete in domains where we know we have good partners. Well, I have one of the contacting fantastic. All Right, thanks for coming on the show again. We look forward to having you back on to talk more about kind of Ava architecture, as well as the plans on building on top of it, because there's a lot more outcrepative right. So this will take a few paths to get it right. You know, I'm sorry about that. Yeah, thank you. Thank you very much for having me. This is a blasts 